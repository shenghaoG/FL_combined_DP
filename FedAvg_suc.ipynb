{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedAvg_suc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shenghaoG/FL_combined_DP/blob/master/FedAvg_suc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTCewGuumiF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-encrypted\n",
        "\n",
        "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
        "\n",
        "!cd PySyft; python setup.py install  > /dev/null\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "!pip install --upgrade --force-reinstall lz4\n",
        "!pip install --upgrade --force-reinstall websocket\n",
        "!pip install --upgrade --force-reinstall websockets\n",
        "!pip install --upgrade --force-reinstall zstd\n",
        "!pip install --upgrade msgpack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxWxBfJntzMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch==1.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwEHhYteuJmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torchvision==0.3.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx1x3cTUWqih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import syft as sy\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-frYVwLWqii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hook = sy.TorchHook(torch)\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
        "secure_worker=sy.VirtualWorker(hook, id=\"secure_worker\")\n",
        "bob.add_workers([alice,secure_worker])\n",
        "alice.add_workers([bob,secure_worker])\n",
        "secure_worker.add_workers([alice,bob])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JF7G8OD5I8nW",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.epochs = 10\n",
        "        self.lr = 0.001\n",
        "        self.momentum = 0.5\n",
        "        self.no_cuda = False\n",
        "        self.seed = 1\n",
        "        self.log_interval = 100\n",
        "        self.save_model = True\n",
        "        self.gauss_mu=0\n",
        "        self.gauss_sigma=0.001\n",
        "\n",
        "args = Arguments()\n",
        "\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sCsPkgTASoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "                          datasets.MNIST('../data', train=True, download=True,\n",
        "                          transform=transforms.Compose([\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize((0.1307,), (0.3081,))])).federate((bob, alice)),\n",
        "                          batch_size=args.batch_size, shuffle=True,**kwargs)\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "print(len(federated_train_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCnEPF-SzhRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
        "        self.fc1 = nn.Linear(4*4*64, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iixkWI7_PlLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "                       datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))])),\n",
        "                       batch_size=args.test_batch_size, shuffle=True)\n",
        "def test(args, model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('Test set: Average loss: {:.8f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    return correct\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckxhEzKuWqik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.epochs = 10\n",
        "import time\n",
        "start=time.time()\n",
        "best_correct_list=[]\n",
        "\n",
        "best_correct = 0\n",
        "model = Net().cuda()\n",
        "model.train()\n",
        "model_bob = model.copy().send(bob)\n",
        "model_alice= model.copy().send(alice)\n",
        "optimizer_bob = optim.SGD(params=model_bob.parameters(), lr=args.lr)\n",
        "optimizer_alice = optim.SGD(params=model_alice.parameters(), lr=args.lr)\n",
        "    \n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        for idx,(data, target) in enumerate(federated_train_loader):\n",
        "            \n",
        "            if data.location == bob:\n",
        "                model_bob.train()\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer_bob.zero_grad()\n",
        "                output_bob = model_bob(data)\n",
        "                loss_bob = F.nll_loss(output_bob, target)\n",
        "                loss_bob.backward()\n",
        "                optimizer_bob.step()\n",
        "                loss_bob = loss_bob.get().data\n",
        "            elif data.location == alice:\n",
        "                model_alice.train()\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                optimizer_alice.zero_grad()\n",
        "                output_alice = model_alice(data)\n",
        "                loss_alice = F.nll_loss(output_alice, target)\n",
        "                loss_alice.backward()\n",
        "                optimizer_alice.step()\n",
        "                loss_alice = loss_alice.get().data\n",
        "\n",
        "#             if(idx % args.log_interval == 0):\n",
        "#                 print('Train Epoch: {} \\tAlice Loss: {:.6f} \\tbob Loss: {:.6f}'.format(epoch, loss_alice.item(), loss_bob.item()))\n",
        "#                 break;\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 model_bob.move(secure_worker)\n",
        "#                 model_alice.move(secure_worker)\n",
        "            \n",
        "#                 model_bob= model_bob.get()\n",
        "#                 model_alice= model_alice.get()\n",
        "                \n",
        "#                 for pram_bob,pram_alice in zip(model_bob.parameters(),model_alice.parameters()):\n",
        "#                     C_bob=torch.median(torch.abs(pram_bob.grad))\n",
        "#                     C_alice=torch.median(torch.abs(pram_alice.grad))\n",
        "#                     grad_bob = torch.normal(pram_bob.grad/max(1, torch.norm(pram_bob.grad)/C_bob),sigma*C_bob)\n",
        "#                     grad_alice =  torch.normal(pram_alice.grad/max(1, torch.norm(pram_alice.grad)/C_alice),sigma*C_alice)            \n",
        "#                     grad_avg = (grad_bob+grad_alice)/2.0\n",
        "#                     pram_bob.grad = grad_avg\n",
        "#                     pram_alice.grad = grad_avg\n",
        "                    \n",
        "#                 optimizer_bob.step()\n",
        "#                 optimizer_alice.step()\n",
        "            \n",
        "#             print(\"test bob_model\")\n",
        "#             test(args, model_bob, device, test_loader)\n",
        "#             print(\"test alice_model\")\n",
        "#             test(args, model_alice, device, test_loader)\n",
        "\n",
        "#             # add DP after averaging random.gauss(0,float(torch.mean(torch.abs(pram1[i].data)))*0.1)\n",
        "#             pram1 = [i for i in model_bob.parameters()]\n",
        "#             pram2 = [i for i in model_alice.parameters()]\n",
        "#             for i in range(len(pram1)):\n",
        "# #                 print(float(torch.mean(torch.abs(pram1[i].data))))\n",
        "#                 temp=((pram1[i] + random.gauss(0,float(torch.mean(torch.abs(pram1[i].data)))*t) + \n",
        "#                        pram2[i] + random.gauss(0,float(torch.mean(torch.abs(pram2[i].data)))*t)) * 0.5).float().to(device)\n",
        "#                 pram1[i].set_(temp)\n",
        "#                 pram2[i].set_(temp)\n",
        "\n",
        "            \n",
        "                \n",
        "            if(idx%20==0):\n",
        "                correct=test(args, model_alice, device, test_loader)\n",
        "                if(correct>best_correct):\n",
        "                    best_correct=correct\n",
        "\n",
        "            model_bob = model_bob.send(bob)\n",
        "            model_alice = model_alice.send(alice)\n",
        "\n",
        "    best_correct_list.append(best_correct/10000)\n",
        "    print('\\n')\n",
        "\n",
        "print(best_correct_list)\n",
        "print(\"done! use time \",(time.time()-start)/60)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}